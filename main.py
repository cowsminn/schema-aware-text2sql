from pydantic import BaseModel, Field
from langchain_community.utilities import SQLDatabase
from langchain_community.tools.sql_database.tool import QuerySQLCheckerTool
from langgraph.graph import START, StateGraph
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from sqlalchemy import create_engine
from langchain_openai import ChatOpenAI
from pathlib import Path  # FIXED: from pathlib import Path
import os
import argparse
import json
import re

# sql lite local, nu am credidentiale pt google cloud(bigquery) sau snowflake(sf_bq)
#100k tokens pe grok, deci aprox 25k cuvinte

load_dotenv()

class State(BaseModel):
    question: str           
    query: str = ""       
    result: str = ""   
    answer: str = ""

class QueryOutput(BaseModel):
    """Generated SQL query."""
    query: str = Field(description="Syntactically valid SQL query.")
    
class FeedbackState(BaseModel):
    """Stare extinsÄƒ cu feedback."""
    question: str
    query: str = ""
    result: str = ""
    answer: str = ""
    feedback_score: int = 0  # 1=pozitiv, 0=neutru, -1=negativ
    feedback_comment: str = ""

class EvaluationResult(BaseModel):
    question: str
    query_ground_truth: str
    query_generated: str
    exact_match: bool
    execution_accuracy: bool

# def get_db():
#     engine = create_engine("postgresql+psycopg2://postgres:testare@localhost:5432/text2sql_db")
#     return SQLDatabase(engine)

def get_db():
    """Get database connection with proper date handling."""
    from sqlalchemy.pool import StaticPool
    from sqlalchemy import event, text
    import os
    
    # Use absolute path to db folder
    db_path = os.path.join(os.path.dirname(__file__), "db", "netflixdb.sqlite")
    
    engine = create_engine(
        f"sqlite:///{db_path}",
        connect_args={'check_same_thread': False},
        poolclass=StaticPool
    )
    
    # Disable automatic date conversion for SQLite
    @event.listens_for(engine, "connect")
    def set_sqlite_pragma(dbapi_conn, connection_record):
        cursor = dbapi_conn.cursor()
        cursor.execute("PRAGMA parse_time=0")
        cursor.close()
    
    # Always use without sample rows to avoid date conversion issues
    return SQLDatabase(engine, sample_rows_in_table_info=0)

system_message = """
You are an expert Text-to-SQL system.

Given a natural language question and a database schema, generate a
syntactically correct SQLite SQL query.

IMPORTANT RULES:
- Use ONLY table and column names that appear EXACTLY in the schema.
- Do NOT invent or pluralize table names.
- Do NOT invent columns.
- If a table or column does not exist, do NOT use it.
- Pay close attention to the data types of columns (boolean, date, integer, varchar, etc.)

BOOLEAN HANDLING:
- For boolean columns, use 1 for TRUE and 0 for FALSE in SQLite
- Example: WHERE available_globally = 1

Database schema (with sample rows):
{table_info}
"""

user_prompt = "Question: {input}"

query_prompt_template = ChatPromptTemplate([
    ("system", system_message), 
    ("user", user_prompt)
])

def get_schema_tables(table_info: str):
    """Extract table names from schema info."""
    # Match both CREATE TABLE and Table: patterns
    tables = set(re.findall(r'CREATE TABLE ["\']?(\w+)["\']?', table_info, re.IGNORECASE))
    tables.update(re.findall(r'Table:\s*["\']?(\w+)["\']?', table_info, re.IGNORECASE))
    return tables

def validate_tables(sql: str, table_info: str):
    """Validate that SQL only uses tables that exist in schema."""
    schema_tables = get_schema_tables(table_info)
    
    # Extract table names from FROM and JOIN clauses
    pattern = r'(?:FROM|JOIN)\s+["`]?(\w+)["`]?'
    used_tables = set(re.findall(pattern, sql, re.IGNORECASE))
    
    # Remove debug prints
    # Case-insensitive comparison
    schema_lower = {t.lower() for t in schema_tables}
    used_lower = {t.lower() for t in used_tables}
    
    is_valid = used_lower.issubset(schema_lower)
    
    if not is_valid:
        invalid_tables = used_lower - schema_lower
        print(f"âŒ Invalid tables found: {invalid_tables}")
    
    return is_valid

def write_query(state: State):
    """Generate SQL query to fetch information."""
    db = get_db()
    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)

    table_info = db.get_table_info()

    prompt = query_prompt_template.invoke({
        "table_info": table_info,
        "input": state.question
    })

    msg = llm.bind_tools([QueryOutput]).invoke(prompt)

    if not msg.tool_calls:
        print("âš ï¸ No tool calls received from LLM")
        state.query = ""
        return {"query": ""}

    candidate = msg.tool_calls[0]["args"]["query"]
    print(f"ðŸ“ Generated SQL: {candidate}")

    if not validate_tables(candidate, table_info):
        print("âš ï¸ Invalid table detected")
        state.query = ""
        return {"query": ""}

    state.query = candidate
    return {"query": state.query}

def write_query_with_llm(state: State, model_type: str = "groq"):
    """Generate SQL with specified LLM."""
    db = get_db()
    llm = get_llm(model_type)
    
    table_info = db.get_table_info()
    
    prompt = query_prompt_template.invoke({
        "table_info": table_info,
        "input": state.question
    })
    
    llm_with_tools = llm.bind_tools([QueryOutput])
    msg = llm_with_tools.invoke(prompt)
    
    if msg.tool_calls:
        candidate = msg.tool_calls[0]["args"]["query"]
        
        if not validate_tables(candidate, table_info):
            print(f"âš ï¸ {model_type}: Invalid table detected")
        
        state.query = candidate
        print(f"SQL generated with {model_type}: {state.query}")
        return {"query": state.query}
    else:
        print(f"Could not generate SQL with {model_type}")
        return {"query": ""}

def execute_query(state: State):
    """Execute SQL query with validation."""
    if not state.query:
        state.result = "No SQL query to execute"
        return {"result": state.result}
    
    try:
        db = get_db()
        llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
        checker_tool = QuerySQLCheckerTool(db=db, llm=llm)
        check_result = checker_tool.invoke(state.query)
        
        if "error" in check_result.lower():
            state.result = f"Invalid SQL query: {check_result}"
            return {"result": state.result}
        
        result = db.run(state.query)
        state.result = result
        print(f"âœ… Results: {result}")
        return {"result": result}
        
    except Exception as e:
        error_msg = f"SQL execution error: {e}"
        state.result = error_msg
        print(error_msg)
        return {"result": error_msg}

def generate_answer(state: State):
    if not state.query or not state.result:
        state.answer = "Unable to answer the question because no valid SQL query could be generated."
        return {"answer": state.answer}

    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
    
    prompt = f"""
    Given the following user question, corresponding SQL query, and SQL result,
    answer the user question in English in a natural and helpful way.
    
    Question: {state.question}
    SQL Query: {state.query}
    SQL Result: {state.result}
    
    Provide a clear, concise answer in English:
    """
    
    response = llm.invoke(prompt)
    state.answer = response.content
    print(f"Final answer: {state.answer}")
    return {"answer": state.answer}

def handle_ambiguous_input(state: State):
    """DetecteazÄƒ È™i gestioneazÄƒ input ambiguu."""
    # Cuvinte cheie care indicÄƒ ambiguitate
    ambiguous_keywords = ["toate", "multe", "puÈ›ine", "recent", "vechi", "popular"]
    
    if any(keyword in state.question.lower() for keyword in ambiguous_keywords):
        print(f"ÃŽntrebarea pare ambiguÄƒ: {state.question}")
        
        clarification = input("PoÈ›i fi mai specific? (ex: ce an, cÃ¢te rezultate, etc.): ")
        state.question = f"{state.question}. {clarification}"
        print(f"ÃŽntrebare clarificatÄƒ: {state.question}")
    
    return state

def get_user_feedback(result: dict) -> dict:
    """Cere feedback de la utilizator."""
    print(f"\nQuestion: {result.get('question', 'N/A')}")
    print(f"Answer: {result.get('answer', 'N/A')}")
    
    feedback = input("\nIs the answer correct? (yes/no/partial): ").lower()
    comment = input("Comments (optional): ")
    
    score = 1 if feedback in ["yes", "da"] else (-1 if feedback in ["no", "nu"] else 0)
    
    return {
        "feedback_score": score,
        "feedback_comment": comment
    }


def handle_negative_feedback(result: dict):
    """Handle negative feedback by regenerating query."""
    print("Trying to improve the answer...")
    
    state = State(question=result.get('question', ''))
    
    write_query(state)
    execute_query(state)
    generate_answer(state)
    
    return {
        "question": state.question,
        "query": state.query,
        "result": state.result,
        "answer": state.answer
    }

def get_llm(model_type: str):
    if model_type == "groq":
        return ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
    elif model_type == "ollama":
        from langchain_ollama import ChatOllama
        return ChatOllama(model="llama3.1:8b", temperature=0)
    elif model_type == "ollama3.2":
        from langchain_ollama import ChatOllama
        return ChatOllama(model="llama3.2", temperature=0)
    elif model_type == "ollama-qwen":
        from langchain_ollama import ChatOllama
        return ChatOllama(model="qwen2.5:7b-instruct", temperature=0)
    elif model_type == "cohere":
        from langchain_cohere import ChatCohere
        return ChatCohere(model="command-r", temperature=0)
    elif model_type == "gemini":
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            temperature=0,
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
    else:
        raise ValueError(f"Unknown model: {model_type}")

def compare_llms(question: str):
    """Compare SQL generation across different LLMs."""
    models = ["groq", "ollama", "ollama3.2", "ollama-qwen", "gemini", "cohere"]
    results = {}
    
    for model in models:
        print(f"\nðŸ”„ Testing {model.upper()}...")
        try:
            state = State(question=question)
            write_query_with_llm(state, model)
            execute_query(state)
            generate_answer(state)
            results[model] = {
                "query": state.query,
                "result": state.result,
                "answer": state.answer
            }
            print(f"âœ… {model.upper()} completed")
        except Exception as e:
            results[model] = {"error": str(e)}
            print(f"âŒ {model.upper()} failed: {e}")
    
    return results

def evaluate_on_spider(spider_json_path: str, max_questions: int = 10):
    """EvalueazÄƒ pe benchmark-ul Spider."""
    with open(spider_json_path, 'r') as f:
        spider_data = json.load(f)
    
    results = []
    
    for i, item in enumerate(spider_data[:max_questions]):
        try:
            state = State(question=item['question'])
            write_query(state)
            
            # Exact Match
            exact_match = state.query.strip().lower() == item['query'].strip().lower()
            
            execute_query(state)
            execution_accuracy = "error" not in state.result.lower()
            
            results.append(EvaluationResult(
                question=item['question'],
                query_ground_truth=item['query'],
                query_generated=state.query,
                exact_match=exact_match,
                execution_accuracy=execution_accuracy
            ))
            
        except Exception as e:
            print(f"Eroare la evaluarea Ã®ntrebÄƒrii {i}: {e}")
    
    return results

def evaluate_on_spider_benchmark(spider_dir: str, max_questions: int = None):
    """Evaluate on official Spider benchmark with proper metrics."""
    # Import doar cÃ¢nd este necesar
    from spider_evaluator import SpiderBenchmarkEvaluator
    
    evaluator = SpiderBenchmarkEvaluator(spider_dir)
    spider_data = evaluator.load_dataset('dev')
    
    if max_questions:
        spider_data = spider_data[:max_questions]
    
    predictions = []
    
    for i, item in enumerate(spider_data):
        print(f"\nðŸ”„ Procesez {i+1}/{len(spider_data)}: {item['question']}")
        
        try:
            # FoloseÈ™te baza de date specificatÄƒ Ã®n Spider
            db_id = item['db_id']
            db_path = evaluator.database_dir / db_id / f"{db_id}.sqlite"
            
            # SchimbÄƒ temporar baza de date
            global get_db
            original_get_db = get_db
            
            def get_spider_db():
                from sqlalchemy import create_engine
                from sqlalchemy.pool import StaticPool
                engine = create_engine(
                    f"sqlite:///{db_path}",
                    connect_args={'check_same_thread': False},
                    poolclass=StaticPool
                )
                return SQLDatabase(engine, sample_rows_in_table_info=0)
            
            get_db = get_spider_db
            
            # GenereazÄƒ SQL
            state = State(question=item['question'])
            write_query(state)
            
            predictions.append({
                'question': item['question'],
                'db_id': db_id,
                'predicted_query': state.query,
                'gold_query': item['query']
            })
            
            # RestabileÈ™te get_db original
            get_db = original_get_db
            
        except Exception as e:
            print(f"âŒ Eroare la Ã®ntrebarea {i}: {e}")
            predictions.append({
                'question': item['question'],
                'db_id': item.get('db_id', 'unknown'),
                'predicted_query': "",
                'gold_query': item['query']
            })
    
    # EvalueazÄƒ toate predicÈ›iile
    metrics = evaluator.evaluate_predictions(predictions)
    
    return metrics, predictions

def load_few_shot_examples(eval_suite_dir: Path, num_examples: int = 3):
    """ÃŽncarcÄƒ cÃ¢teva exemple gold SQL pentru few-shot learning."""
    gold_sql_dir = eval_suite_dir / "gold" / "sql"
    
    few_shot_examples = []
    
    # SelecteazÄƒ exemple gold SQL
    gold_files = list(gold_sql_dir.glob("local*.sql"))[:num_examples]
    
    for sql_file in gold_files:
        instance_id = sql_file.stem
        sql_content = sql_file.read_text()
        
        # ÃŽncearcÄƒ sÄƒ gÄƒseascÄƒ Ã®ntrebarea corespunzÄƒtoare
        few_shot_examples.append({
            'instance_id': instance_id,
            'sql': sql_content
        })
    
    return few_shot_examples

def evaluate_on_spider2_lite(spider2_lite_dir: str, max_questions: int = None, primary_llm: str = "groq"):
    """EvalueazÄƒ pe Spider2-Lite - genereazÄƒ predicÈ›ii È™i ruleazÄƒ scriptul oficial de evaluare."""
    from pathlib import Path
    
    spider2_lite_path = Path(spider2_lite_dir).resolve()
    data_file = spider2_lite_path / "spider2-lite.jsonl"
    eval_script = spider2_lite_path / "evaluation_suite" / "evaluate.py"
    eval_suite_dir = spider2_lite_path / "evaluation_suite"
    
    # VerificÄƒ bazele de date SQLite
    db_dir_localdb = spider2_lite_path / "resource" / "databases" / "spider2-localdb"
    db_dir_direct = spider2_lite_path / "resource" / "databases"
    
    # Alege locaÈ›ia care are cele mai multe fiÈ™iere .sqlite
    if db_dir_localdb.exists():
        localdb_count = len(list(db_dir_localdb.glob("*.sqlite")))
    else:
        localdb_count = 0
    
    if db_dir_direct.exists():
        direct_count = len(list(db_dir_direct.glob("*.sqlite")))
    else:
        direct_count = 0
    
    if localdb_count > direct_count:
        db_dir = db_dir_localdb
        print(f"âš ï¸  Bazele de date sunt Ã®n spider2-localdb/, dar evaluatorul le cautÄƒ Ã®n databases/")
        print(f"ðŸ’¡ RuleazÄƒ: cd {db_dir_direct} && ln -s spider2-localdb/*.sqlite .")
    else:
        db_dir = db_dir_direct
    
    # VerificÄƒ bazele de date SQLite
    if db_dir.exists():
        sqlite_files = list(db_dir.glob("*.sqlite"))
        print(f"ðŸ“‚ Baze de date disponibile Ã®n {db_dir}:")
        print(f"   GÄƒsite {len(sqlite_files)} fiÈ™iere .sqlite")
        if sqlite_files:
            print(f"   Exemple: {', '.join([f.name for f in sqlite_files[:5]])}...")
    else:
        print(f"âš ï¸  Directorul {db_dir} nu existÄƒ!")
        return None
    
    # ÃŽncarcÄƒ few-shot examples
    print(f"\nðŸ“š ÃŽncÄƒrcare few-shot examples...")
    few_shot_examples = load_few_shot_examples(eval_suite_dir, num_examples=2)
    print(f"âœ… ÃŽncÄƒrcate {len(few_shot_examples)} exemple gold SQL")
    
    # ÃŽncarcÄƒ date
    print(f"\nðŸ“– ÃŽncÄƒrcare date din {data_file}...")
    data = []
    with open(data_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"âœ… ÃŽncÄƒrcate {len(data)} exemple total")
    
    # FiltreazÄƒ doar exemple care au baze de date SQLite disponibile
    sqlite_data = []
    available_dbs = {f.stem for f in sqlite_files}
    
    for item in data:
        db_name = item['db']
        if db_name.startswith(('bigquery', 'snowflake', 'ga4', 'ga360', 'firebase')):
            continue
        
        if db_name in available_dbs:
            sqlite_data.append(item)
    
    print(f"âœ… Filtrate {len(sqlite_data)} exemple cu baze de date SQLite disponibile")
    
    if max_questions:
        sqlite_data = sqlite_data[:max_questions]
        print(f"âœ… Limitate la {len(sqlite_data)} exemple pentru testare")
    
    if not sqlite_data:
        print("âŒ Nu existÄƒ exemple cu baze de date SQLite disponibile!")
        return None
    
    # GenereazÄƒ predicÈ›ii
    predictions = []
    llm_fallback = None  # Pentru fallback la alt LLM
    
    for i, item in enumerate(sqlite_data):
        instance_id = item['instance_id']
        db_id = item['db']
        question = item['question']
        
        print(f"\nðŸ”„ [{i+1}/{len(sqlite_data)}] {instance_id}")
        print(f"   DB: {db_id}")
        print(f"   Q: {question[:80]}...")
        
        db_file = db_dir / f"{db_id}.sqlite"
        
        if not db_file.exists():
            print(f"   âš ï¸  DB nu existÄƒ: {db_file}")
            predictions.append({"instance_id": instance_id, "SQL": ""})
            continue
        
        try:
            import sqlite3
            
            conn = sqlite3.connect(str(db_file))
            cursor = conn.cursor()
            
            # ObÈ›ine lista de tabele
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [row[0] for row in cursor.fetchall()]
            
            # ObÈ›ine schema completÄƒ pentru fiecare tabelÄƒ
            schema_info = []
            for table in tables:
                cursor.execute(f"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}';")
                create_stmt = cursor.fetchone()
                if create_stmt:
                    schema_info.append(create_stmt[0])
                    
                    # AdaugÄƒ È™i cÃ¢teva rÃ¢nduri exemplu
                    cursor.execute(f"SELECT * FROM {table} LIMIT 2;")
                    sample_rows = cursor.fetchall()
                    if sample_rows:
                        cursor.execute(f"PRAGMA table_info({table});")
                        columns = [col[1] for col in cursor.fetchall()]
                        schema_info.append(f"-- Sample data: {columns[:5]}...")
            
            conn.close()
            
            schema_text = "\n\n".join(schema_info)
            print(f"   ðŸ“‹ Schema: {len(tables)} tabele: {', '.join(tables[:5])}...")
            
            # ConstruieÈ™te few-shot examples text
            few_shot_text = ""
            if few_shot_examples:
                few_shot_text = "\n\nHere are some example SQL queries from similar tasks:\n\n"
                for ex in few_shot_examples:
                    few_shot_text += f"Example {ex['instance_id']}:\n{ex['sql'][:300]}...\n\n"
            
            # Prompt Ã®mbunÄƒtÄƒÈ›it cu few-shot È™i instrucÈ›iuni detaliate
            prompt_text = f"""You are an expert SQL query generator specializing in complex analytical queries.

CRITICAL INSTRUCTIONS:
1. Use ONLY these exact table names: {', '.join(tables)}
2. Write standard SQL compatible with SQLite
3. Pay attention to:
   - Use NTILE() for percentile-based scoring when needed
   - Use window functions (OVER, PARTITION BY) for advanced analytics
   - Use proper date functions (DATE(), STRFTIME())
   - Use JOINs with USING() when columns have same names
   - Group and aggregate data appropriately
4. Return ONLY the complete SQL query, nothing else (no explanations, markdown, or comments outside the SQL)

DATABASE SCHEMA:
{schema_text}
{few_shot_text}

QUESTION: {question}

Generate a complete, executable SQL query:"""
            
            # ÃŽncearcÄƒ LLM-ul principal, apoi fallback la Gemini
            generated_sql = None
            
            try:
                # FoloseÈ™te LLM-ul selectat de utilizator
                if primary_llm == "groq":
                    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
                elif primary_llm == "gemini":
                    from langchain_google_genai import ChatGoogleGenerativeAI
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-2.0-flash-exp",
                        temperature=0,
                        google_api_key=os.getenv("GOOGLE_API_KEY")
                    )
                else:
                    llm = get_llm(primary_llm)
                
                response = llm.invoke(prompt_text)
                generated_sql = response.content.strip()
                
            except Exception as primary_error:
                if "rate_limit" in str(primary_error).lower() or "429" in str(primary_error):
                    print(f"   âš ï¸  {primary_llm.upper()} rate limit - folosesc Gemini fallback")
                    
                    # Fallback la Gemini (dacÄƒ nu e deja Gemini)
                    if llm_fallback is None and primary_llm != "gemini":
                        from langchain_google_genai import ChatGoogleGenerativeAI
                        llm_fallback = ChatGoogleGenerativeAI(
                            model="gemini-2.0-flash-exp",
                            temperature=0,
                            google_api_key=os.getenv("GOOGLE_API_KEY")
                        )
                    
                    if llm_fallback:
                        response = llm_fallback.invoke(prompt_text)
                        generated_sql = response.content.strip()
                    else:
                        raise primary_error
                else:
                    raise primary_error
            
            # CurÄƒÈ›Äƒ SQL-ul
            generated_sql = re.sub(r'^```[\w]*', '', generated_sql, flags=re.MULTILINE)
            generated_sql = re.sub(r'```$', '', generated_sql, flags=re.MULTILINE)
            generated_sql = re.sub(r'^(SQL|Query):\s*', '', generated_sql, flags=re.IGNORECASE | re.MULTILINE)
            generated_sql = '\n'.join(line for line in generated_sql.split('\n') if line.strip())
            generated_sql = generated_sql.strip()
            
            # Self-validation: Ã®ncearcÄƒ sÄƒ rulezi SQL-ul pentru a verifica sintaxa
            try:
                test_conn = sqlite3.connect(str(db_file))
                test_cursor = test_conn.cursor()
                test_cursor.execute(f"EXPLAIN QUERY PLAN {generated_sql}")
                test_conn.close()
                print(f"   âœ… SQL valid (verificat cu EXPLAIN)")
            except Exception as validation_error:
                print(f"   âš ï¸  SQL ar putea avea probleme: {str(validation_error)[:100]}")
            
            print(f"   âœ… SQL: {generated_sql[:80]}...")
            
            predictions.append({
                "instance_id": instance_id,
                "SQL": generated_sql
            })
            
        except Exception as e:
            print(f"   âŒ Eroare: {e}")
            predictions.append({
                "instance_id": instance_id,
                "SQL": ""
            })
    
    # SalveazÄƒ predicÈ›iile
    pred_file = Path("spider2_lite_predictions.jsonl")
    pred_dir = Path("spider2_lite_predictions")
    
    with open(pred_file, 'w', encoding='utf-8') as f:
        for pred in predictions:
            f.write(json.dumps(pred, ensure_ascii=False) + '\n')
    
    print(f"\nðŸ’¾ PredicÈ›ii salvate: {pred_file}")
    
    # CreeazÄƒ director È™i salveazÄƒ fiecare predicÈ›ie ca fiÈ™ier .sql separat
    pred_dir.mkdir(exist_ok=True)
    
    for pred in predictions:
        instance_id = pred['instance_id']
        sql_content = pred['SQL']
        sql_file = pred_dir / f"{instance_id}.sql"
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write(sql_content)
    
    print(f"ðŸ’¾ FiÈ™iere SQL salvate Ã®n: {pred_dir}/ ({len(predictions)} fiÈ™iere)")
    
    # RuleazÄƒ scriptul oficial de evaluare
    print("\n" + "="*60)
    print("ðŸ“Š RULEAZÄ‚ EVALUAREA OFICIALÄ‚ SPIDER2-LITE")
    print("="*60)
    
    import subprocess
    import sys
    
    cmd = [
        sys.executable,
        str(eval_script.resolve()),
        '--mode', 'sql',
        '--result_dir', str(pred_dir.absolute())
    ]
    
    print(f"ComandÄƒ: {' '.join(cmd)}\n")
    print(f"â³ AÈ™teptare evaluare (poate dura cÃ¢teva minute)...\n")
    
    try:
        result = subprocess.run(
            cmd,
            cwd=str(eval_script.parent.resolve()),
            capture_output=True,
            text=True,
            timeout=300  # ADDED: 5 minute timeout
        )
        
        print(result.stdout)
        if result.stderr:
            print(f"Stderr:\n{result.stderr}")
        
        if result.returncode != 0:
            print(f"\nâš ï¸  Scriptul de evaluare a returnat cod {result.returncode}")
        
        # ParseazÄƒ scorul din output
        score_match = re.search(r'Final score:\s*([\d.]+)', result.stdout)
        final_score = float(score_match.group(1)) if score_match else 0.0
        
        return {
            'predictions': predictions,
            'predictions_file': str(pred_file),
            'predictions_dir': str(pred_dir),
            'evaluation_output': result.stdout,
            'return_code': result.returncode,
            'final_score': final_score
        }
        
    except subprocess.TimeoutExpired:
        print(f"â±ï¸  Evaluarea a depÄƒÈ™it timeout-ul de 5 minute!")
        print(f"\nðŸ’¡ RuleazÄƒ manual:")
        print(f"   cd {eval_script.parent}")
        print(f"   python evaluate.py --mode sql --result_dir {pred_dir.absolute()}")
        
        return {
            'predictions': predictions,
            'predictions_file': str(pred_file),
            'predictions_dir': str(pred_dir),
            'timeout': True
        }
    except Exception as e:
        print(f"âš ï¸  Eroare la evaluare: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'predictions': predictions,
            'predictions_file': str(pred_file),
            'predictions_dir': str(pred_dir)
        }

def calculate_metrics(results: list):
    """CalculeazÄƒ metricile de evaluare."""
    total = len(results)
    exact_matches = sum(r.exact_match for r in results)
    execution_accuracies = sum(r.execution_accuracy for r in results)
    
    return {
        "exact_match_accuracy": exact_matches / total if total > 0 else 0,
        "execution_accuracy": execution_accuracies / total if total > 0 else 0,
        "total_questions": total
    }
        
def create_text2sql_graph():
    """CreeazÄƒ un LangGraph StateGraph pentru pipeline-ul Text-to-SQL."""
    graph_builder = StateGraph(State)
    
    # AdaugÄƒ nodurile Ã®n secvenÈ›Äƒ
    graph_builder.add_sequence([write_query, execute_query, generate_answer])
    graph_builder.add_edge(START, "write_query")
    
    return graph_builder.compile()

def run_pipeline(question: str):
    graph = create_text2sql_graph()
    result = graph.invoke({"question": question})
    return result

def run_pipeline_with_feedback(question: str):
    """RuleazÄƒ pipeline-ul cu feedback loop complet."""
    result = run_pipeline(question)
    feedback = get_user_feedback(result)
    
    # DacÄƒ feedback-ul e negativ, Ã®ncearcÄƒ sÄƒ Ã®mbunÄƒtÄƒÈ›eÈ™ti
    if feedback['feedback_score'] < 0:
        print("Trying to improve the answer based on your feedback...")
        improved_result = handle_negative_feedback(result)
        
        # Cere feedback din nou pentru rÄƒspunsul Ã®mbunÄƒtÄƒÈ›it
        print("\n--- IMPROVED ANSWER ---")
        print(f"New Answer: {improved_result.get('answer', 'N/A')}")
        final_feedback = get_user_feedback(improved_result)
        
        return improved_result, final_feedback
    
    return result, feedback

def load_spider_dataset(spider_dir: str):
    """Load Spider dataset and database schemas."""
    with open(os.path.join(spider_dir, "dev.json"), "r") as f:
        dev_data = json.load(f)
    with open(os.path.join(spider_dir, "tables.json"), "r") as f:
        tables_data = json.load(f)
    return dev_data, tables_data

def get_spider_db_path(spider_dir: str, db_id: str):
    """Get the path to the SQLite database for a given Spider database ID."""
    return os.path.join(spider_dir, "database", db_id, f"{db_id}.sqlite")

def evaluate_spider_dataset(spider_dir: str, max_questions: int = None):
    """Evaluate the Text-to-SQL pipeline on the Spider dataset."""
    dev_data, tables_data = load_spider_dataset(spider_dir)
    if max_questions:
        dev_data = dev_data[:max_questions]

    results = []
    for i, item in enumerate(dev_data):
        print(f"Evaluating question {i + 1}/{len(dev_data)}: {item['question']}")
        db_id = item["db_id"]
        db_path = get_spider_db_path(spider_dir, db_id)

        # Temporarily override the get_db function to use the Spider database
        global get_db
        original_get_db = get_db

        def get_spider_db():
            from sqlalchemy import create_engine
            from sqlalchemy.pool import StaticPool
            engine = create_engine(
                f"sqlite:///{db_path}",
                connect_args={"check_same_thread": False},
                poolclass=StaticPool
            )
            return SQLDatabase(engine, sample_rows_in_table_info=0)

        get_db = get_spider_db

        try:
            state = State(question=item["question"])
            write_query(state)

            # Check exact match
            exact_match = state.query.strip().lower() == item["query"].strip().lower()

            execute_query(state)
            execution_accuracy = "error" not in state.result.lower()

            results.append(EvaluationResult(
                question=item["question"],
                query_ground_truth=item["query"],
                query_generated=state.query,
                exact_match=exact_match,
                execution_accuracy=execution_accuracy
            ))
        except Exception as e:
            print(f"Error evaluating question {i + 1}: {e}")
        finally:
            # Restore the original get_db function
            get_db = original_get_db

    return results

def main():
    parser = argparse.ArgumentParser(description="InterfaÈ›Äƒ Text-to-SQL")
    parser.add_argument("--question", type=str, help="ÃŽntrebare Ã®n limbaj natural")
    parser.add_argument("--compare", action="store_true", help="ComparÄƒ LLM-uri diferite")
    parser.add_argument("--interactive", action="store_true", help="Mod interactiv")
    parser.add_argument("--feedback", action="store_true", help="ActiveazÄƒ feedback loop")
    parser.add_argument("--evaluate", type=str, help="Calea cÄƒtre fiÈ™ierul JSON Spider")
    parser.add_argument("--spider-benchmark", type=str, help="Calea cÄƒtre directorul Spider")
    parser.add_argument("--spider2-lite", type=str, help="Calea cÄƒtre directorul Spider2-Lite")
    parser.add_argument("--max-questions", type=int, default=None, help="NumÄƒr maxim de Ã®ntrebÄƒri")
    parser.add_argument("--llm", type=str, default="groq", 
                       choices=["groq", "ollama", "ollama3.2", "ollama-qwen", "gemini", "cohere"],
                       help="LLM de folosit pentru generarea SQL (default: groq)")
     
    args = parser.parse_args()
    
    # AfiÈ™eazÄƒ LLM-ul selectat
    if not args.compare:
        print(f"ðŸ¤– LLM selectat: {args.llm.upper()}")
    
    if args.spider2_lite:
        print("ðŸš€ RuleazÄƒ evaluarea Spider2-Lite...")
        result = evaluate_on_spider2_lite(
            args.spider2_lite, 
            args.max_questions,
            primary_llm=args.llm
        )
        
        if not result:
            print("\nâŒ Evaluarea a eÈ™uat - verificÄƒ bazele de date SQLite")
            return
        
        print("\n" + "="*60)
        print("âœ… EVALUARE COMPLETÄ‚")
        print("="*60)
        print(f"PredicÈ›ii salvate Ã®n: {result['predictions_file']}")
        print(f"Total predicÈ›ii: {len(result['predictions'])}")
        
        if 'final_score' in result:
            print(f"\nðŸŽ¯ SCOR FINAL: {result['final_score']:.2%}")
        
        if 'evaluation_output' in result and result['evaluation_output']:
            print("\nðŸ“Š REZULTATE EVALUARE OFICIALÄ‚:")
            for line in result['evaluation_output'].split('\n'):
                if 'Final score' in line or 'Correct examples' in line or 'Total examples' in line:
                    print(f"   {line}")
        
        return
    
    if args.spider_benchmark:
        print("ðŸš€ RuleazÄƒ evaluarea Spider Benchmark...")
        metrics, predictions = evaluate_on_spider_benchmark(
            args.spider_benchmark, 
            args.max_questions
        )
        
        print("\n" + "="*60)
        print("ðŸ“Š REZULTATE SPIDER BENCHMARK")
        print("="*60)
        print(f"Exact Match Accuracy: {metrics['exact_match_accuracy']:.2%}")
        print(f"Execution Accuracy: {metrics['execution_accuracy']:.2%}")
        print(f"Total ÃŽntrebÄƒri: {metrics['total']}")
        
        print("\nðŸ“ˆ Pe Nivel de Dificultate:")
        for level in ['easy', 'medium', 'hard', 'extra']:
            stats = metrics['by_difficulty'][level]
            if stats['count'] > 0:
                print(f"\n{level.upper()}:")
                print(f"  NumÄƒr: {stats['count']}")
                print(f"  Exact Match: {stats['exact_pct']:.2%}")
                print(f"  Execution: {stats['exec_pct']:.2%}")
        
        output_file = "spider_evaluation_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metrics': metrics,
                'predictions': predictions
            }, f, indent=2, ensure_ascii=False)
        print(f"\nðŸ’¾ Rezultate salvate Ã®n {output_file}")
        
        return
    
    if args.evaluate:
        results = evaluate_on_spider(args.evaluate, args.max_questions or 10)
        metrics = calculate_metrics(results)
        print("\n=== Rezultate Evaluare ===")
        print(f"Exact Match Accuracy: {metrics['exact_match_accuracy']:.2%}")
        print(f"Execution Accuracy: {metrics['execution_accuracy']:.2%}")
        print(f"Total ÃŽntrebÄƒri: {metrics['total_questions']}")
        return
        
    if args.interactive:
        while True:
            question = input("\nIntroduceÈ›i Ã®ntrebarea (sau 'exit'): ")
            if question.lower() == 'exit':
                break
            
            state = State(question=question)
            write_query_with_llm(state, args.llm)
            execute_query(state)
            
            if args.feedback:
                feedback = get_user_feedback({"question": question, "answer": state.answer})
                print(f"Feedback salvat: {feedback}")
                
                if feedback['feedback_score'] < 0:
                    print("Trying to improve the answer...")
                    write_query_with_llm(state, args.llm)
                    execute_query(state)
            else:
                print(f"\nðŸ“Š SQL: {state.query}")
                print(f"ðŸ“Š Result: {state.result}")
    
    elif args.compare:
        question = args.question or input("ÃŽntrebare pentru comparaÈ›ie: ")
        results = compare_llms(question)
        
        for model, result in results.items():
            print(f"\n=== {model.upper()} ===")
            print(f"Query: {result.get('query', 'N/A')}")
            print(f"RÄƒspuns: {result.get('answer', 'N/A')}")
    
    else:
        question = args.question or input("ÃŽntrebare: ")
        state = State(question=question)
        write_query_with_llm(state, args.llm)
        
        if args.feedback:
            execute_query(state)
            generate_answer(state)
            feedback = get_user_feedback({"question": question, "answer": state.answer})
            print(f"Feedback: {feedback}")
        else:
            execute_query(state)
            print(f"\nðŸ“Š SQL: {state.query}")
            print(f"ðŸ“Š Result: {state.result}")

if __name__ == "__main__":
    main()